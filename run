#!/usr/bin/env python
import sys
sys.dont_write_bytecode = True

import os
from os.path import join as pjoin

import subprocess
import tempfile
import atexit

import argparse
import secrets


# TODO: assertion on no defined SLURM_JOB_ID
SLURM_LOCAL_DATA = '/local-data/user-data/$USER/$SLURM_JOB_ID'


def print_blue(*args, **kwargs):
    print('\033[1;34m', end='')
    print(*args, **kwargs)
    print('\033[0m', end='')


def print_red(*args, **kwargs):
    print('\033[1;31m', end='')
    print(*args, **kwargs)
    print('\033[0m', end='')


def print_green(*args, **kwargs):
    print('\033[1;32m', end='')
    print(*args, **kwargs)
    print('\033[0m', end='')


def raise_die(msg: str, code: int=1):
    print_red(msg)
    sys.exit(code)


def randomize(length: int=8) -> str:
    return secrets.token_urlsafe(length)


def join_newline(lst, num_newlines=1):
    sep = '\n' * num_newlines if num_newlines > 0 else ' '
    return sep.join(lst)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description='A one-liner for GSAI slurm sbatch scripts')

    env_group = parser.add_mutually_exclusive_group()
    env_group.add_argument('--venv', action='store_true', help='Use the default virtual environment')
    env_group.add_argument('--conda', type=str, help='Use a Conda environment with the given name')
    env_group.add_argument('--docker', type=str, help='Use a Docker image with the given name')
    env_group.add_argument('--singularity', type=str, help='Use a Singularity image with the given file name')

    parser.add_argument('--jobname', type=str, required=True, help='Name of the job')
    parser.add_argument('--partition', type=str, required=True, help='Slurm partition to use')
    parser.add_argument('--n_gpus', type=int, required=True, help='Number of GPUs to request')
    parser.add_argument('--n_cpus', type=int, required=True, help='Number of CPUs to request')
    parser.add_argument('--time', type=str, required=True, help='Time limit for the job (e.g., 01:00:00 for 1 hour)')

    parser.add_argument('--precopy', nargs=2, action='append', metavar=('NAME', 'SRC'), required=False, default=[], help='Specify a source and destination pair for file pre-copying')
    parser.add_argument('--postcopy', nargs=2, action='append', metavar=('NAME', 'DST'), required=False, default=[], help='Specify a source and destination pair for file post-copying')

    parser.add_argument('command', nargs='+', help='The command to execute')

    args = parser.parse_args()
    return args


def copy_cleanups(args: argparse.Namespace) -> tuple[str, str, str, dict[str, str]]:
    pre_copy = []
    post_copy = []
    cleanup = []
    mappings = {}

    _names = [n for n, _ in (args.precopy + args.postcopy)]
    if len(_names) != len(set(_names)):
        raise_die('Error: Duplicate names found in precopy/postcopy arguments')

    precp_prefix = pjoin(SLURM_LOCAL_DATA, 'PRE')
    postcp_prefix = pjoin(SLURM_LOCAL_DATA, 'POST')

    pre_copy.append(f'mkdir -p "{precp_prefix}"')
    cleanup.append(f'rm -drf "{SLURM_LOCAL_DATA}"')
    post_copy.append(f'mkdir -p "{postcp_prefix}"')

    for precp_name, precp_src in (args.precopy):
        precp_src = precp_src.rstrip('/')

        if not os.path.exists(precp_src):
            raise_die(f'Precopy source path does not exist: {precp_src}')

        src_parent, src_name = os.path.split(precp_src)
        path = pjoin(precp_prefix, randomize())
        mappings[precp_name] = pjoin(path, src_name)

        _pre_copy = []
        _pre_copy.append(f'mkdir -p "{path}"')
        _pre_copy.append(f'tar -cf - -C "{src_parent}" "{src_name}" | tar -xf - -C "{path}"')
        pre_copy.extend(_pre_copy)

    for postcp_name, postcp_dst in (args.postcopy):
        postcp_dst = postcp_dst.rstrip('/')

        path = pjoin(postcp_prefix, randomize())
        mappings[postcp_name] = path

        _post_copy = []
        _post_copy.append(f'mkdir -p "{postcp_dst}"')
        _post_copy.append(f'tar -cf - -C "{path}" . | tar -xf - -C "{postcp_dst}"')
        post_copy.extend(_post_copy)

    return (
        join_newline(pre_copy),
        join_newline(post_copy),
        join_newline(cleanup),
        mappings
    )


def write_sbatch_script(args: argparse.Namespace) -> str:
    status_code = join_newline([
        '#!/bin/bash',
        'cd $SLURM_SUBMIT_DIR',
        'echo "SLURM_JOB_NAME=$SLURM_JOB_NAME"',
        'echo "SLURM_JOB_ID=$SLURM_JOB_ID"',
        'echo "SLURM_SUBMIT_DIR=$SLURM_SUBMIT_DIR"',
        'echo "HOSTNAME: $(hostname)"',
        'echo "DATE: $(date)"',
        'echo "%%%%%%%%%%%% GPU INFO %%%%%%%%%%%%"',
        'echo "CUDA_HOME=$CUDA_HOME"',
        'echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"',
        'echo "CUDA_VERSION=$CUDA_VERSION"',
        'nvidia-smi',
        'nvidia-smi -L',
    ])

    sbatch_script = tempfile.NamedTemporaryFile(mode='w', delete=False)
    script_path = sbatch_script.name
    atexit.register(os.remove, script_path)

    if args.venv:
        run_prefix = 'source .venv/bin/activate; which python; '
    elif args.conda:
        run_prefix = f'conda activate {args.conda}; which python; '
    elif args.docker:
        # _docker_setup = '  '
        _docker_setup = join_newline([
            'export UUIDLIST=$(nvidia-smi -L | cut -d \'(\' -f 2 | awk \'{print$2}\' | tr -d ")" | paste -s -d, -)',
            'export GPULIST="device=${UUIDLIST}"',
            'export CPULIST=$(grep "Cpus_allowed_list" /proc/self/status | awk \'{print $2}\');'
        ])
        _docker_rand = randomize()
        run_prefix = f'docker run --rm --pull always --init --gpus $GPULIST --cpuset-cpus $CPULIST -e WANDB_API_KEY=$WANDB_API_KEY --volume $(pwd):/workspace --name $(whoami)_{_docker_rand} {args.docker} '
        run_prefix = join_newline([_docker_setup, run_prefix])
    elif args.singularity:
        _singularity_setup = 'ml singularity; '
        run_prefix = f'singularity exec --nv --env WANDB_API_KEY=$WANDB_API_KEY --bind $(pwd):/workspace {args.singularity} '
        run_prefix = _singularity_setup + run_prefix
    else:
        run_prefix = ''
    
    precopy, postcopy, cleanup, copy_lookup = copy_cleanups(args)
    parsed_command = join_newline(args.command, num_newlines=0)
    for name, path in copy_lookup.items():
        parsed_command = parsed_command.replace('{{' + name + '}}', f'"{path}"')
    exec_cmd = run_prefix + parsed_command

    script_contents = []
    script_contents.append(status_code)
    script_contents.append('echo " *** Precopying *** "')
    script_contents.append(precopy)
    script_contents.append('echo " *** Executing command *** "')
    script_contents.append(exec_cmd)
    script_contents.append('echo " *** Postcopying *** "')
    script_contents.append(postcopy)
    script_contents.append('echo " *** Cleaning-up *** "')
    script_contents.append(cleanup)
    
    script_contents = join_newline(script_contents, num_newlines=2)
    sbatch_script.write(script_contents)
    sbatch_script.close()

    print_green('Wrote the following sbatch script:')
    print(script_contents, end='\n\n')
    print_green('to:', end=' ')
    print(script_path, end='\n\n')

    return script_path


# TODO: use an early singal (using --signal) to clean up in case of job termination
def execute_sbatch(script_path: str, args: argparse.Namespace):
    sbatch_args = []
    sbatch_args.append(('--job-name', args.jobname))
    sbatch_args.append(('--out', f'$HOME/slurm_log/out/$job_name.{args.jobname}.out'))
    sbatch_args.append(('--err', f'$HOME/slurm_log/err/$job_name.{args.jobname}.err'))
    sbatch_args.append(('--partition', args.partition))
    sbatch_args.append(('--nodes', '1'))  # TODO: support multi-node distributed jobs
    sbatch_args.append(('--ntasks', '1'))
    sbatch_args.append(('--gres', f'gpu:{args.n_gpus}'))
    sbatch_args.append(('--cpus-per-task', args.n_cpus))
    sbatch_args.append(('--time', args.time))

    if 'A100' in args.partition and 'PCI' not in args.partition:
        sbatch_args.append(('qos', 'hpgpu'))

    sbatch_args = join_newline([f'{k} {v}' for k, v in sbatch_args], num_newlines=0)
    sbatch_command = f'sbatch {sbatch_args} {script_path}'

    print_green('Running the following sbatch command:')
    print(sbatch_command, end='\n\n')

    subprocess.run(sbatch_command, shell=True)


if __name__ == '__main__':
    args = parse_args()
    path = write_sbatch_script(args)
    execute_sbatch(path, args)
else:
    raise_die("This script cannot be imported as a module")
